{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Prediction of Poisonous Mushrooms\n",
    "\n",
    "### Artificial Intelligence 2nd Project\n",
    "\n",
    "The aim of this project is to implement and evaluate machine learning models for predicting whether a mushroom is **poisonous** or **edible** based on its physical characteristics. As such, this is a binary classification problem, where the target variable is the venomosity of the mushroom.\n",
    "\n",
    "To achieve our goal, we will follow the standard machine learning pipeline, which consists of analyzing the data, preprocessing it to ensure higher accuracy, and, finally, training and comparing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Coding environment](#coding-environment)\n",
    "    1. [Importing the Libraries](#importing-the-libraries)\n",
    "    2. [Loading the Dataset](#loading-the-dataset)\n",
    "2. [Initial Data Exploration](#initial-data-exploration)\n",
    "3. [Data Cleaning](#data-cleaning)\n",
    "    1. [Exploring the Dataset](#exploring-the-dataset)\n",
    "    2. [Removing Duplicates](#removing-duplicates)\n",
    "    3. [Filling in Missing Values](#filling-in-missing-values)\n",
    "    4. [Removing Outliers](#remove-outliers)\n",
    "    5. [Encoding Qualitative Data](#encoding-qualitative-data)\n",
    "4. [Training the Models](#training-the-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding environment\n",
    "\n",
    "### Importing the libraries\n",
    "\n",
    "Due to its extensive machine learning ecosystem, we have opted to use [Python](https://www.python.org/) for this project. As such, before proceeding, it is imperative to prepare our coding environment by importing the libraries we will be working with, namely:\n",
    "\n",
    "* **[Pandas](https://pandas.pydata.org/)** - For data manipulation and preprocessing.\n",
    "* **[Scikit-learn](https://scikit-learn.org/stable/)** - For implementing machine learning models and evaluation metrics.\n",
    "* **[Matplotlib](https://matplotlib.org/)** - For creating graphs, tables, and numerous other data visualization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "Next, we must load the data itself, which is stored in a compressed CSV file. However, there is no need to manually uncompress it, as Pandas handles that automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Exploration\n",
    "\n",
    "It is evident that having a solid understanding of the data is paramount to training accurate models. Therefore, having finished the setup, the first step is to **explore** the dataset to uncover its characteristics. To start, below is a small excerpt from our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First rows from our dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, it is clear that the `id` column offers no significant information, as it simply indicates the index of the corresponding row. As such, we can safely drop it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, we should analyze the data types of the remaining columns to get a better picture of the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, our dataset contains 21 columns and over 3 million rows. Regarding the columns, only three contain **quantitative** data, whereas the remaining 18 pertain to **qualitative** data. To facilitate further analysis, we will categorize and extract the columns into distinct variables based on their data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'class'\n",
    "\n",
    "# compute the quantitative columns\n",
    "quantitative_columns = df.select_dtypes(include=['number']).columns\n",
    "print(\"Quantitative columns: \", quantitative_columns.tolist())\n",
    "\n",
    "# compute the qualitative columns EXCEPT the target column\n",
    "qualitative_columns = df.select_dtypes(include=['object']).columns.drop(target_column)\n",
    "print(\"\\nQualitative columns: \", qualitative_columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a succint description of each column:\n",
    "\n",
    "| **Column/Feature**         | **Description**                                                                 |\n",
    "|:--------------------------:|:-------------------------------------------------------------------------------:|\n",
    "| `class`                    | Binary label that Indicates if the mushroom is edible or poisonous.             |\n",
    "| `cap-diameter`             | Width of the cap at its widest point, in centimeters.                           |\n",
    "| `cap-shape`                | General form of the cap—e.g., flat, conical, bell-shaped, or wavy.              |\n",
    "| `cap-surface`              | Texture of the cap—smooth, scaly, sticky, or wrinkled.                          |\n",
    "| `cap-color`                | Cap color, which may change as the mushroom matures.                            |\n",
    "| `does-bruise-or-bleed`     | Indicates if the mushroom bruises or releases liquid when damaged.              |\n",
    "| `gill-attachment`          | How gills connect to the stem—free, attached, or descending.                    |\n",
    "| `gill-spacing`             | Distance between gills—crowded, spaced, or intermediate.                        |\n",
    "| `gill-color`               | Color of the gills, which may change with age.                                  |\n",
    "| `stem-height`              | Length of the stem from base to cap, in centimeters.                            |\n",
    "| `stem-width`               | Thickness of the stem—narrow, medium, or thick, in centimeters.                 |\n",
    "| `stem-root`                | Shape of the stem’s base—tapered, swollen, or bulbous.                          |\n",
    "| `stem-surface`             | Stem texture—smooth, fibrous, scaly, or rough.                                  |\n",
    "| `stem-color`               | Color of the stem, which may be uniform or variable.                            |\n",
    "| `veil-type`                | Indicates a partial or universal veil.                                          |\n",
    "| `veil-color`               | Color of the veil, useful for identification.                                   |\n",
    "| `has-ring`                 | Indicates if a ring (annulus) is present on the stem.                           |\n",
    "| `ring-type`                | Type of ring—single, double, flaring, or hanging.                               |\n",
    "| `spore-print-color`        | Color of spores left on a surface; key for identification.                      |\n",
    "| `habitat`                  | Environment where the mushroom grows—e.g., woods or grasslands.                 |\n",
    "| `season`                   | Time of year when the mushroom is typically found.                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now that we have a basic understanding of the dataset, we must prepare it for analysis by addressing any inconsistencies, errors, or missing values that could skew our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "\n",
    "Firstly, it is important to determine if the dataset contains **duplicate** rows, as those can be safely excluded without affecting the accuracy of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset contains {} duplicates.\".format(df.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset contains no duplicates, that means all rows provide relevant information, so none must be removed.\n",
    "\n",
    "### Filling in Missing Values\n",
    "\n",
    "Another key concern has to do with **missing values**, that is, entries that are absent from the dataset. Seeing as these provide no information, it might be sensible to either delete the rows where they appear or replace the missing entries with meaningful values.\n",
    "\n",
    "The following highlights the amount of missing values per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values per column (%):\")\n",
    "pd.DataFrame({\n",
    "    'Count': df.isna().sum(),\n",
    "    'Frequency (%)': 100 * df.isna().mean(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.title(\"Visualizing Missing Values\")\n",
    "sns.heatmap(df.isnull(), cbar=False, yticklabels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the dataset has an overabundance of missing values, with some columns having over half of its entries missing. Because of this, we will opt to **fill in** the missing values, as removing the rows where they appear would result in a tremendous data loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Quantitative Data\n",
    "\n",
    "There are several methods to fill in missing numerical data. However, the most appropriate for each column depends on the **distribution** of its values:\n",
    "\n",
    "* If the values are symmetrically distributed, it is appropriate to fill the missing entries with the **mean** as it represents the central tendency more accurately.\n",
    "* If the values are asymmetrically distributed (**skewed**), then the **median** should be used because it is less affected by outliers.\n",
    "\n",
    "The following depicts the distribution of each quantitative column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the skewness\n",
    "print(\"Skewness by column:\")\n",
    "print(df[quantitative_columns].skew())\n",
    "\n",
    "# plot the distribution\n",
    "plt.figure(figsize=(4 * len(quantitative_columns), 4))\n",
    "\n",
    "for index, column in enumerate(quantitative_columns):\n",
    "    plt.subplot(1, len(quantitative_columns), index+1)\n",
    "    sns.histplot(data=df, x=column, kde=True, bins=20, stat='probability')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.ylabel('Frequency')\n",
    "    sns.despine()\n",
    "\n",
    "plt.tight_layout() # adjust subplots to fit into figure area\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering all quantitative columns are right-skewed, we must fill their missing values with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in quantitative_columns:\n",
    "    # compute the median of the column's values\n",
    "    median = df[column].median()\n",
    "\n",
    "    # fill the missing values with the median\n",
    "    df[column] = df[column].fillna(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative Data\n",
    "\n",
    "Handling missing values in qualitative data requires imputation strategies that consider the nature of the data, such as using the **mode**, creating a new **category**, or employing more advanced techniques based on relationships within the data.\n",
    "\n",
    "As became apparent in ..., there are plenty of qualitative columns where over half the entries are missing (`stem-root`, `veil-type`, `veil-color`, etc.), but there are also a few where only a small percentage is absent (`cap-shape`, `cap-color`, `does-bruise-or-bleed`, etc). So, we will take this into account when replacing the missing entries:\n",
    "* If more than a predetermined percentage of data is missing, we create a new category - `Unspecified` - to group these unspecified values.\n",
    "* Otherwise, we fill the missing values with the column's mode so as to preserve the distribution as much as possible.\n",
    "\n",
    "As for the threshold, we believe 1% will help preserve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_qualitative_data(data: pd.Series, threshold: int) -> pd.Series:\n",
    "    '''Fills missing qualitative data based on the number of missing values.'''\n",
    "    missing_values = data.isna().sum() / len(data)\n",
    "    mode = data.mode()\n",
    "\n",
    "    return data.fillna('Unspecified' if missing_values > threshold or mode.empty else mode[0])\n",
    "\n",
    "\n",
    "# replace the missing values with the mean\n",
    "for column in qualitative_columns:\n",
    "    df[column] = fill_missing_qualitative_data(df[column], 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Upon cleaning the dataset, the next step is to analyze the underlying patterns and relationships within the data.\n",
    "\n",
    "### Detecting Outliers\n",
    "\n",
    "Having ensured all dataset entries have a value, it is appropriate to handle any **outliers** to avoid training our models with unrepresentative data.\n",
    "\n",
    "#### Quantitative Data\n",
    "\n",
    "To detect outliers in quantitative data, we can start by plotting the box plot of the respective columns as this type of graph is ideal for easily identifying extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_box_plots(df, quantitative_columns):\n",
    "    '''Plots the distribution of the quantitative columns.'''\n",
    "    plt.figure(figsize=(4 * len(quantitative_columns), 4))\n",
    "\n",
    "    for index, column in enumerate(quantitative_columns):\n",
    "        plt.subplot(1, len(quantitative_columns), index+1)\n",
    "        sns.boxplot(data=df, x=column)\n",
    "        plt.title(f'Box plot of {column}')\n",
    "        sns.despine()\n",
    "\n",
    "    plt.tight_layout() # adjust subplots to fit into figure area\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot the distributions\n",
    "display_box_plots(df, quantitative_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we can conclude that there are several outliers. However, in order to decide how to deal with them, we need to understand just how many there are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(data: pd.Series, lower_quantile: float) -> pd.Series:\n",
    "    '''Computes the outliers of a given data column.'''\n",
    "    Q1 = data.quantile(lower_quantile)\n",
    "    Q3 = data.quantile(1 - lower_quantile)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    return data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "\n",
    "# calculate the percentage of outliers for each quantitative column\n",
    "print(\"Outliers by column (%):\")\n",
    "\n",
    "for column in quantitative_columns:\n",
    "    outliers = get_outliers(df[column], 0.25)\n",
    "    outliers_percentage = 100 * len(outliers) / len(df[column])\n",
    "\n",
    "    print(f'{column}\\t{round(outliers_percentage, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced, with $Q_{0.25}$ and $Q_{0.75}$ as lower and upper bounds, each quantitative column contains fewer than 5% outliers. Therefore, removing the rows where they appear would not incur a severe data loss for individual columns. However, assuming the worst-case scenario of only one outlier per row, we would be losing over 8% of the dataset, which is not ideal.\n",
    "\n",
    "One possible solution would be to reduce the threshold and obtain fewer outliers. However, considering our quantitative columns are right-skewed, we believe applying a variance-stabilizing transformation should be more fruitful, as it would not only reduce the impact of large values but also normalize the distribution of our columns. With that in mind, we will apply a **logarithmic transformation** to the quantitative columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply logarithmic transformation\n",
    "for column in quantitative_columns:\n",
    "    df[column] = np.log1p(df[column])\n",
    "\n",
    "# view the updated box plots\n",
    "display_box_plots(df, quantitative_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative Data\n",
    "\n",
    "Even though outliers are commonly associated with quantitative data, they can also occur in qualitative data. In this case, the term refers to categories that are atypically infrequent i.e. that appear significantly less than the others.\n",
    "\n",
    "To identify these values, we will compute the percentage of each category per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in qualitative_columns:\n",
    "    # compute the frequency of each category\n",
    "    freqs = pd.DataFrame({\n",
    "        'Count': df[column].value_counts(),\n",
    "        'Frequency (%)': df[column].value_counts(normalize=True) * 100\n",
    "    })\n",
    "\n",
    "    print(f\"\\nCategory Frequency in '{column}'\")\n",
    "    print(freqs.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As depicted, some categories appear very infrequently ($< 0.01\\%$). Despite the desire to include all unique categories for greater fidelity, keeping these rare values in the dataset may introduce noise, as they are unlikely to provide meaningful patterns and can instead take focus away from the dominant trends.\n",
    "\n",
    "An obvious solution to this problem would be to remove the rows containing these categories, which would naturally result in data loss. However, we can take advantage of the fact that we have a category for missing values - `Unspecified` - and use it to encapsulate these atypical categories, thus achieving the desired effect while preserving all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_infrequent_qualitative_data(data: pd.Series, freq_threshold: float)-> pd.Series:\n",
    "    '''Replaces rare categorical values with 'Unspecified'.'''\n",
    "    # compute the frequency of each category\n",
    "    infrequent = data.value_counts(normalize=True)[lambda x : x * 100 < freq_threshold].index\n",
    "\n",
    "    # replace the infrequent values\n",
    "    return data.apply(lambda x: 'Unspecified' if x in infrequent else x)\n",
    "\n",
    "\n",
    "# count the number of unique categories per column\n",
    "before_unique_categories = df[qualitative_columns].nunique()\n",
    "\n",
    "# replace the infrequent values in each column\n",
    "for column in qualitative_columns:\n",
    "    df[column] = replace_infrequent_qualitative_data(df[column], 0.01)\n",
    "\n",
    "# compare the number of unique categories before and after processing\n",
    "print(\"Unique Categories per column:\")\n",
    "pd.DataFrame({\n",
    "    'Before': before_unique_categories,\n",
    "    'After': df[qualitative_columns].nunique()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Correlations\n",
    "\n",
    "Next, we will identify possible relationships between features by computing their correlations with each other and with the target variable. This analysis will deepen our understanding of the dataset and may also help us eliminate highly collinear features, reducing redundancy and improving model performance.\n",
    "\n",
    "#### Quantitative vs Quantitative\n",
    "\n",
    "To examine the strength between pairs of quantitative features, we will compute the **Pearson correlation coefficient**, which is a statistical measure that quantifies how two variables move together. It ranges from -1 (perfect negative correlation) to 1 (perfect positive correlation). The results are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[quantitative_columns].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated, the maximum correlation between quantitative variables pertains to `cap-diameter` and `stem-width` and is $88\\%$. So, if the two are similarly related to the target variable, it might be wise to merge them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantitative vs Target\n",
    "\n",
    "Next, we will determine how each quantitative variable relates to the target variable. To that end, we will use the **Point-Biserial Correlation Coefficient**, which is a particular case of Pearson's correlation coefficient designed for comparing continuous variables with a binary variable. Similarly to Pearson's, the coefficient varies from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the target variable\n",
    "encoded_target = df[target_column].map({ 'e': 0, 'p': 1})\n",
    "\n",
    "# compute the correlations\n",
    "correlations = {}\n",
    "\n",
    "for column in quantitative_columns:\n",
    "    corr_coeff, _ = stats.pointbiserialr(df[column], encoded_target)\n",
    "    correlations[column] = [corr_coeff] # store as a list to create a DataFrame row\n",
    "\n",
    "# convert the correlations to a DataFrame\n",
    "correlation_df = pd.DataFrame(correlations, index=['class']).T\n",
    "\n",
    "# plot the correlation matrix\n",
    "sns.heatmap(correlation_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As denoted, all quantitative features are weakly correlated with the target variable. Nevertheless, both `cap-diameter` and `stem-width` are identically related to the target, which, adding to the fact they are strongly correlated with each other, further corroborates that we should join them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative vs Target\n",
    "\n",
    "The final correlations we will compute are between the qualitative features and the target class. To accomplish this, we will use **mosaic plots** to directly visualize how each category in a qualitative column is related to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calculates Cramer's V statistic for a given confusion matrix.\n",
    "    confusion_matrix: A 2D numpy array or pandas DataFrame representing the contingency table.\n",
    "    \"\"\"\n",
    "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    min_dim = min(confusion_matrix.shape) - 1\n",
    "    \n",
    "    # Handle the case where min_dim is 0 to avoid division by zero\n",
    "    if min_dim == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate Cramer's V, ensuring no division by zero for n\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return np.sqrt(chi2 / (n * min_dim))\n",
    "\n",
    "results = []\n",
    "\n",
    "for column in qualitative_columns:\n",
    "    print(f\"\\nAnalyzing '{column}' vs 'target':\")\n",
    "    \n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(df[column], df[target_column])\n",
    "    print(\"Contingency Table:\")\n",
    "    print(contingency_table)\n",
    "    \n",
    "    # Perform Chi-squared test\n",
    "    chi2, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "    print(f\"Chi-squared statistic: {chi2:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    \n",
    "    # Calculate Cramer's V\n",
    "    v_value = cramers_v(contingency_table)\n",
    "    print(f\"Cramer's V: {v_value:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'Feature': column,\n",
    "        'Chi2_Statistic': chi2,\n",
    "        'P_Value': p_value,\n",
    "        'Cramers_V': v_value\n",
    "    })\n",
    "\n",
    "# Summarize results in a DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by='Cramers_V', ascending=False)\n",
    "print(\"\\n--- Summary of Qualitative Feature Associations ---\")\n",
    "print(results_df)\n",
    "\n",
    "# --- 4. Visualize with Stacked Bar Charts ---\n",
    "print(\"\\n--- Visualizing Associations with Stacked Bar Charts ---\")\n",
    "\n",
    "for col in qualitative_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create normalized contingency table for percentages\n",
    "    # This shows the proportion of 0s and 1s within each category of 'col'\n",
    "    normalized_contingency = pd.crosstab(df[col], df[target_column], normalize='index')\n",
    "    \n",
    "    # Plotting\n",
    "    normalized_contingency.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())\n",
    "    \n",
    "    plt.title(f'Target Distribution by {col} (Normalized)')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Target', labels=['0', '1'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the Dataset\n",
    "\n",
    "The final exploratory step concerns the distribution of the target variable itself. More specifically, it is crucial to verify if the dataset is **balanced**, that is, whether there is a roughly equal number of edible and poisonous mushroom samples. This is necessary because training models on unbalanced data can cause them to inadvertedly be biased towards the most prevalent class.\n",
    "\n",
    "The following showcases the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df[target_column].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(class_counts, labels=[\"Edible\", \"Poisonous\"], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the percentage of edible and poisonous mushrooms differs by less than $10\\%$, the dataset is reasonably balanced, so there is no need to perform any additional steps to alter the target value distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Qualitative Data\n",
    "\n",
    "As most machine learning algorithms and statistical models can only process numerical input, the final preprocessing step is transforming qualitative data into quantitative data.\n",
    "\n",
    "There are several encoding techniques that achieve this, but we will go with **label encoding**, which merely consists of assigning unique integer values to distinct categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the label encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# apply Label Encoding to all qualitative data\n",
    "for column in df[qualitative_columns]:\n",
    "    df[column] = encoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and target (y)\n",
    "y = df['class']\n",
    "X = df.drop('class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y, scoring_metrics: list[str], cv: int):\n",
    "    '''\n",
    "    Evaluates a list of models using cross-validation and returns a Pandas DataFrame\n",
    "    with the mean scores for each model.\n",
    "    '''\n",
    "\n",
    "    # initialize the results\n",
    "    results = {}\n",
    "\n",
    "    for model in models:\n",
    "        model_name = type(model).__name__\n",
    "\n",
    "        # perform cross-validation\n",
    "        scores = cross_validate(model, X, y, scoring=scoring_metrics, cv=cv)\n",
    "\n",
    "        # calculate the mean of each scoring metric\n",
    "        results[model_name] = {\n",
    "            metric: np.mean(scores[f\"test_{metric}\"]) for metric in scoring_metrics\n",
    "        }\n",
    "\n",
    "    # convert the results to a Pandas data frame\n",
    "    return pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "models = [\n",
    "    DecisionTreeClassifier(random_state=21),\n",
    "]\n",
    "\n",
    "# define the scoring metrics\n",
    "scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "# define the scoring metrics\n",
    "evaluate_models(models, X, y, scoring_metrics, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
