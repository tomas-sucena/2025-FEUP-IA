{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Prediction of Poisonous Mushrooms\n",
    "\n",
    "### Artificial Intelligence 2nd Project\n",
    "\n",
    "The aim of this project is to implement and evaluate machine learning models for predicting whether a mushroom is **poisonous** or **edible** based on its physical characteristics. As such, this is a binary classification problem, where the target variable is the venomosity of the mushroom.\n",
    "\n",
    "To achieve our goal, we will follow the standard machine learning pipeline, which consists of analyzing the data, preprocessing it to ensure higher accuracy, and, finally, training and comparing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Coding environment](#coding-environment)\n",
    "    1. [Importing the Libraries](#importing-the-libraries)\n",
    "    2. [Loading the Dataset](#loading-the-dataset)\n",
    "2. [Data Analysis and Preprocessing](#data-analysis-and-preprocessing)\n",
    "    1. [Exploring the Dataset](#exploring-the-dataset)\n",
    "    2. [Removing Duplicates](#removing-duplicates)\n",
    "    3. [Filling in Missing Values](#filling-in-missing-values)\n",
    "    4. [Removing Outliers](#remove-outliers)\n",
    "    5. [Encoding Qualitative Data](#encoding-qualitative-data)\n",
    "3. [Training the Models](#training-the-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding environment\n",
    "\n",
    "### Importing the libraries\n",
    "\n",
    "Due to its extensive machine learning ecosystem, we have opted to use [Python](https://www.python.org/) for this project. As such, before proceeding, it is imperative to prepare our coding environment by importing the libraries we will be working with, namely:\n",
    "\n",
    "* **[Pandas](https://pandas.pydata.org/)** - For data manipulation and preprocessing.\n",
    "* **[Scikit-learn](https://scikit-learn.org/stable/)** - For implementing machine learning models and evaluation metrics.\n",
    "* **[Matplotlib](https://matplotlib.org/)** - For creating graphs, tables, and numerous other data visualization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "Next, we must load the data itself, which is stored in a compressed CSV file. However, there is no need to manually uncompress it, as Pandas handles that automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Exploration\n",
    "\n",
    "It is evident that having a solid understanding of the data is paramount to training accurate models. Therefore, having finished the setup, the first step is to **explore** the dataset to uncover its characteristics. To start, below is a small excerpt from our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First rows from our dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, it is clear that the `id` column offers no significant information, as it simply indicates the index of the corresponding row. As such, we can safely drop it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, we should analyze the data types of the remaining columns to get a better picture of the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, our dataset contains 21 columns and over 3 million rows. Regarding the columns, only three contain **quantitative** data, whereas the remaining 18 pertain to **qualitative** data. To facilitate further analysis, we will categorize and extract the columns into distinct variables based on their data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'class'\n",
    "\n",
    "# compute the quantitative columns\n",
    "quantitative_columns = df.select_dtypes(include=['number']).columns\n",
    "print(\"Quantitative columns: \", quantitative_columns.tolist())\n",
    "\n",
    "# compute the qualitative columns EXCEPT the target column\n",
    "qualitative_columns = df.select_dtypes(include=['object']).columns.drop(target_column)\n",
    "print(\"\\nQualitative columns: \", qualitative_columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now that we have a basic understanding of the dataset, we must prepare it for analysis by addressing any inconsistencies, errors, or missing values that could skew our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "\n",
    "Firstly, it is important to determine if the dataset contains **duplicate** rows, as those can be safely excluded without affecting the accuracy of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset contains {} duplicates.\".format(df.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset contains no duplicates, that means all rows provide relevant information, so none must be removed.\n",
    "\n",
    "### Filling in Missing Values\n",
    "\n",
    "Another key concern has to do with **missing values**, that is, entries that are absent from the dataset. Seeing as these provide no information, it might be sensible to either delete the rows where they appear or replace the missing entries with meaningful values.\n",
    "\n",
    "The following highlights the amount of missing values per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values per column (%):\")\n",
    "pd.DataFrame({\n",
    "    'Count': df.isna().sum(),\n",
    "    'Frequency (%)': 100 * df.isna().mean(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.title(\"Visualizing Missing Values\")\n",
    "sns.heatmap(df.isnull(), cbar=False, yticklabels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the dataset has an overabundance of missing values, with some columns having over half of its entries missing. Because of this, we will opt to **fill in** the missing values, as removing the rows where they appear would result in a tremendous data loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Quantitative Data\n",
    "\n",
    "There are several methods to fill in missing numerical data. However, the most appropriate for each column depends on the **distribution** of its values:\n",
    "\n",
    "* If the values are symmetrically distributed, it is appropriate to fill the missing entries with the **mean** as it represents the central tendency more accurately.\n",
    "* If the values are asymmetrically distributed (**skewed**), then the **median** should be used because it is less affected by outliers.\n",
    "\n",
    "The following depicts the distribution of each quantitative column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the skewness\n",
    "print(\"Skewness by column:\")\n",
    "print(df[quantitative_columns].skew())\n",
    "\n",
    "# plot the distribution\n",
    "plt.figure(figsize=(4 * len(quantitative_columns), 4))\n",
    "\n",
    "for index, column in enumerate(quantitative_columns):\n",
    "    plt.subplot(1, len(quantitative_columns), index+1)\n",
    "    sns.histplot(data=df, x=column, kde=True, bins=20, stat='probability')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.ylabel('Frequency')\n",
    "    sns.despine()\n",
    "\n",
    "plt.tight_layout() # adjust subplots to fit into figure area\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering all quantitative columns are right-skewed, we must fill their missing values with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in quantitative_columns:\n",
    "    # compute the median of the column's values\n",
    "    median = df[column].median()\n",
    "\n",
    "    # fill the missing values with the median\n",
    "    df[column] = df[column].fillna(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative Data\n",
    "\n",
    "Handling missing values in qualitative data requires imputation strategies that consider the nature of the data, such as using the **mode**, creating a new **category**, or employing more advanced techniques based on relationships within the data.\n",
    "\n",
    "As became apparent in ..., there are plenty of qualitative columns where over half the entries are missing (`stem-root`, `veil-type`, `veil-color`, etc.), but there are also a few where only a small percentage is absent (`cap-shape`, `cap-color`, `does-bruise-or-bleed`, etc). So, we will take this into account when replacing the missing entries:\n",
    "* If more than a predetermined percentage of data is missing, we create a new category - `Unspecified` - to group these unspecified values.\n",
    "* Otherwise, we fill the missing values with the column's mode so as to preserve the distribution as much as possible.\n",
    "\n",
    "As for the threshold, we believe 1% will help preserve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_qualitative_data(data: pd.Series, threshold: int) -> pd.Series:\n",
    "    '''Fills missing qualitative data based on the number of missing values.'''\n",
    "    missing_values = data.isna().sum() / len(data)\n",
    "    mode = data.mode()\n",
    "\n",
    "    return data.fillna('Unspecified' if missing_values > threshold or mode.empty else mode[0])\n",
    "\n",
    "\n",
    "# replace the missing values with the mean\n",
    "for column in qualitative_columns:\n",
    "    df[column] = fill_missing_qualitative_data(df[column], 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers\n",
    "\n",
    "Having ensured all dataset entries have a value, it is appropriate to remove any **outliers** to avoid training our models with unrepresentative data.\n",
    "\n",
    "#### Quantitative Data\n",
    "\n",
    "To detect outliers in quantitative data, we can start by plotting the box plot of the respective columns as this type of graph is ideal for easily identifying extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_box_plots(df, quantitative_columns):\n",
    "    '''Plots the distribution of the quantitative columns.'''\n",
    "    plt.figure(figsize=(4 * len(quantitative_columns), 4))\n",
    "\n",
    "    for index, column in enumerate(quantitative_columns):\n",
    "        plt.subplot(1, len(quantitative_columns), index+1)\n",
    "        sns.boxplot(data=df, x=column)\n",
    "        plt.title(f'Box plot of {column}')\n",
    "        sns.despine()\n",
    "\n",
    "    plt.tight_layout() # adjust subplots to fit into figure area\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot the distributions\n",
    "display_box_plots(df, quantitative_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we can conclude that there are several outliers. However, in order to decide how to deal with them, we need to understand just how many there are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(data: pd.Series, lower_quantile: float) -> pd.Series:\n",
    "    '''Computes the outliers of a given data column.'''\n",
    "    Q1 = data.quantile(lower_quantile)\n",
    "    Q3 = data.quantile(1 - lower_quantile)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    return data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "\n",
    "# calculate the percentage of outliers for each quantitative column\n",
    "print(\"Outliers by column (%):\")\n",
    "\n",
    "for column in quantitative_columns:\n",
    "    outliers = get_outliers(df[column], 0.25)\n",
    "    outliers_percentage = 100 * len(outliers) / len(df[column])\n",
    "\n",
    "    print(f'{column}\\t{round(outliers_percentage, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced, with $Q_{0.25}$ and $Q_{0.75}$ as lower and upper bounds, each quantitative column contains fewer than 5% outliers. Therefore, removing the rows where they appear would not incur a severe data loss for individual columns. However, assuming the worst-case scenario of only one outlier per row, we would be losing over 8% of the dataset, which is not ideal.\n",
    "\n",
    "One possible solution would be to reduce the threshold and obtain fewer outliers. However, considering our quantitative columns are right-skewed, we believe applying a variance-stabilizing transformation should be more fruitful, as it would not only reduce the impact of large values but also normalize the distribution of our columns. With that in mind, we will apply a **logarithmic transformation** to the quantitative columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply logarithmic transformation\n",
    "for column in quantitative_columns:\n",
    "    df[column] = np.log1p(df[column])\n",
    "\n",
    "# view the updated box plots\n",
    "display_box_plots(df, quantitative_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative Data\n",
    "\n",
    "Even though outliers are commonly associated with quantitative data, they can also occur in qualitative data. In this case, the term refers to categories that are atypically infrequent i.e. that appear significantly less than the others.\n",
    "\n",
    "To identify these values, we will compute the percentage of each category per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in qualitative_columns:\n",
    "    # compute the frequency of each category\n",
    "    freqs = pd.DataFrame({\n",
    "        'Count': df[column].value_counts(),\n",
    "        'Frequency (%)': df[column].value_counts(normalize=True) * 100\n",
    "    })\n",
    "\n",
    "    print(f\"\\nCategory Frequency in '{column}'\")\n",
    "    print(freqs.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As depicted, some categories appear very infrequently ($< 0.01\\%$). Despite the desire to include all unique categories for greater fidelity, keeping these rare values in the dataset may introduce noise, as they are unlikely to provide meaningful patterns and can instead take focus away from the dominant trends.\n",
    "\n",
    "An obvious solution to this problem would be to remove the rows containing these categories, which would naturally result in data loss. However, we can take advantage of the fact that we have a category for missing values - `Unspecified` - and use it to encapsulate these atypical categories, thus achieving the desired effect while preserving all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_infrequent_qualitative_data(data: pd.Series, freq_threshold: float)-> pd.Series:\n",
    "    '''Replaces rare categorical values with 'Unspecified'.'''\n",
    "    # compute the frequency of each category\n",
    "    infrequent = data.value_counts(normalize=True)[lambda x : x * 100 < freq_threshold].index\n",
    "\n",
    "    # replace the infrequent values\n",
    "    return data.apply(lambda x: 'Unspecified' if x in infrequent else x)\n",
    "\n",
    "\n",
    "# count the number of unique categories per column\n",
    "before_unique_categories = df[qualitative_columns].nunique()\n",
    "\n",
    "# replace the infrequent values in each column\n",
    "for column in qualitative_columns:\n",
    "    df[column] = replace_infrequent_qualitative_data(df[column], 0.01)\n",
    "\n",
    "# compare the number of unique categories before and after processing\n",
    "print(\"Unique Categories per column:\")\n",
    "pd.DataFrame({\n",
    "    'Before': before_unique_categories,\n",
    "    'After': df[qualitative_columns].nunique()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Even though there are only three quantitative columns, it is still advantageous to compute their correlation, as it may justify removing columns that are highly correlated with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[quantitative_columns].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated, the maximum correlation between quantitative pertains to `cap-diameter` and `stem-width` and is $75\\%$, which is not a particularly high value. Interestingly, however, both of these columns share a correlation of $36\\%$ with the third column - `stem-height` - which could indicate their similarity. Nevertheless, we believe these reasons are not sufficient to justify merging the two columns, as the marginal benefit of reducing dimensionality does not outweigh the potential loss of predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Qualitative Data\n",
    "\n",
    "As most machine learning algorithms and statistical models can only process numerical input, the final preprocessing step is transforming qualitative data into quantitative data.\n",
    "\n",
    "There are several encoding techniques that achieve this, but we will go with **label encoding**, which merely consists of assigning unique integer values to distinct categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the label encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# apply Label Encoding to all qualitative data\n",
    "for column in df[qualitative_columns]:\n",
    "    df[column] = encoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the Class Distribution\n",
    "\n",
    "The final preprocessing step concerns the distribution of the target variable (`class`, in our case). More specifically, it is crucial to verify if the dataset is **balanced**, that is, whether there is a roughly equal number of edible and poisonous mushroom samples. This is necessary because training models on unbalanced data can cause them to inadvertedly be biased towards the most prevalent class.\n",
    "\n",
    "The following showcases the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['class'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(class_counts, labels=[\"Edible\", \"Poisonous\"], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Classes')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the percentage of edible and poisonous mushrooms only differs by roughly $10\\%$, the dataset is reasonably balanced, so there is no need to perform any additional steps to alter the target value distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and target (y)\n",
    "y = df['class']\n",
    "X = df.drop('class', axis=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = DecisionTreeClassifier(random_state=21)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
