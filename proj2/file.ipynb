{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Prediction of Poisonous Mushrooms\n",
    "\n",
    "### Artificial Intelligence 2nd Project\n",
    "\n",
    "The aim of this project is to implement and evaluate machine learning models for predicting whether a mushroom is **poisonous** or **edible** based on its physical characteristics. As such, this is a binary classification problem, where the target variable is the venomosity of the mushroom.\n",
    "\n",
    "To achieve our goal, we will follow the standard machine learning pipeline, which consists of analyzing the data, preprocessing it to ensure higher accuracy, and, finally, training and comparing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Coding environment](#coding-environment)\n",
    "    1. [Importing the Libraries](#importing-the-libraries)\n",
    "    2. [Loading the Dataset](#loading-the-dataset)\n",
    "2. [Data Analysis and Preprocessing](#data-analysis-and-preprocessing)\n",
    "    1. [Exploring the Dataset](#exploring-the-dataset)\n",
    "    2. [Removing Duplicates](#removing-duplicates)\n",
    "    3. [Filling in Missing Values](#filling-in-missing-values)\n",
    "    4. [Removing Outliers](#remove-outliers)\n",
    "    5. [Encoding Qualitative Data](#encoding-qualitative-data)\n",
    "3. [Training the Models](#training-the-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding environment\n",
    "\n",
    "### Importing the libraries\n",
    "\n",
    "Due to its extensive machine learning ecosystem, we have opted to use [Python](https://www.python.org/) for this project. As such, before proceeding, it is imperative to prepare our coding environment by importing the libraries we will be working with, namely:\n",
    "\n",
    "* **[Pandas](https://pandas.pydata.org/)** - For data manipulation and preprocessing.\n",
    "* **[Scikit-learn](https://scikit-learn.org/stable/)** - For implementing machine learning models and evaluation metrics.\n",
    "* **[Matplotlib](https://matplotlib.org/)** - For creating graphs, tables, and numerous other data visualization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "Next, we must load the data itself, which is stored in a compressed CSV file. However, there is no need to manually uncompress it, as Pandas handles that automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Preprocessing\n",
    "\n",
    "Having finished the setup, the following steps are to **analyze** and **preprocess** the dataset. While it is common to separate the two, we opted to apply the preprocessing as soon as we deem it necessary during our analysis. We believe this decision will enable extra preprocessing opportunities based on the insights gained during the initial exploration.\n",
    "\n",
    "### Exploring the dataset\n",
    "\n",
    "It goes without saying that a solid understanding of the dataset is paramount to training accurate models. Below is a small excerpt from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First rows from our dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, it is clear that the `id` column offers no significant information, as it simply indicates the index of a row. As such, we can safely drop it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, we should analyze the data types of the remaining columns to get a better picture of the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, our dataset contains 21 columns and over 3 million rows. Regarding the columns, only three contain **quantitative** data, whereas the remaining 18 pertain to **qualitative** data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "\n",
    "Next, it is important to determine if the dataset contains **duplicate** rows, as those can be safely excluded without affecting the accuracy of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset contains {} duplicates.\".format(df.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset contains no duplicates, that means all rows provide relevant information, so none must be removed.\n",
    "\n",
    "### Filling in Missing Values\n",
    "\n",
    "Another key concern has to do with **missing values**, that is, entries that are absent from the dataset. Seeing as these provide no information, it might be sensible to either delete the rows where they appear or replace the missing entries with meaningful values.\n",
    "\n",
    "The following highlights the amount of missing values per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values per column (%):\")\n",
    "pd.DataFrame({\n",
    "    'Count': df.isna().sum(),\n",
    "    'Frequency (%)': 100 * df.isna().mean(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "plt.title(\"Visualizing Missing Values\")\n",
    "sns.heatmap(df.isnull(), cbar=False, yticklabels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the dataset has an overabundance of missing values, with some columns having over half of its entries missing. Because of this, we will opt to to **fill in** the missing values, as removing the rows where they appear would result in a tremendous data loss.\n",
    "\n",
    "Given our dataset has both quantitative and qualitative data, we have to deal with them separately. To facilitate this, we will categorize and extract the columns into distinct variables based on their data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'class'\n",
    "\n",
    "# compute the quantitative columns\n",
    "quantitative_columns = df.select_dtypes(include=['number']).columns\n",
    "print(\"Quantitative columns: \", quantitative_columns.tolist())\n",
    "\n",
    "# compute the qualitative columns EXCEPT the target column\n",
    "qualitative_columns = df.select_dtypes(include=['object']).columns.drop(target_column)\n",
    "print(\"\\nQualitative columns: \", qualitative_columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Quantitative Data\n",
    "\n",
    "There are several methods to fill in missing numerical data. However, the most appropriate for each column depends on the **distribution** of its values:\n",
    "\n",
    "* If the values are symmetrically distributed, it is appropriate to fill the missing entries with the **mean** as it represents the central tendency more accurately.\n",
    "* If the values are asymmetrically distributed (**skewed**), then the **median** should be used because it is less affected by outliers.\n",
    "\n",
    "The following depicts the distribution of each quantitative column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the skewness\n",
    "print(\"Skewness by column:\")\n",
    "print(df[quantitative_columns].skew())\n",
    "\n",
    "# plot the distribution\n",
    "plt.figure(figsize=(4 * len(quantitative_columns), 4))\n",
    "\n",
    "for index, column in enumerate(quantitative_columns):\n",
    "    plt.subplot(1, len(quantitative_columns), index+1)\n",
    "    sns.histplot(data=df, x=column, kde=True, bins=20, stat='probability')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.ylabel('Frequency')\n",
    "    sns.despine()\n",
    "\n",
    "plt.tight_layout() # adjust subplots to fit into figure area\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering all quantitative columns are right-skewed, we must fill their missing values with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in quantitative_columns:\n",
    "    # compute the median of the column's values\n",
    "    median = df[column].median()\n",
    "\n",
    "    # fill the missing values with the median\n",
    "    df[column] = df[column].fillna(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative Data\n",
    "\n",
    "Handling missing values in qualitative data requires imputation strategies that consider the nature of the data, such as using the **mode**, creating a new **category**, or employing more advanced techniques based on relationships within the data.\n",
    "\n",
    "As became apparent in ..., there are plenty of qualitative columns where over half the entries are missing (`stem-root`, `veil-type`, `veil-color`, etc.), but there are also a few where only a small percentage is absent (`cap-shape`, `cap-color`, `does-bruise-or-bleed`, etc). So, we will take this into account when replacing the missing entries:\n",
    "* If more than a predetermined percentage of data is missing, we create a new category - `Unspecified` - to group these unspecified values.\n",
    "* Otherwise, we fill the missing values with the column's mode so as to preserve the distribution as much as possible.\n",
    "\n",
    "As for the threshold, we believe 1% will help preserve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_qualitative_data(data: pd.Series, threshold: int) -> pd.Series:\n",
    "    '''Fills missing qualitative data based on the number of missing values.'''\n",
    "    missing_values = data.isna().sum() / len(data)\n",
    "    mode = data.mode()\n",
    "\n",
    "    return data.fillna('Unspecified' if missing_values > threshold or mode.empty else mode[0])\n",
    "\n",
    "\n",
    "# replace the missing values with the mean\n",
    "for column in qualitative_columns:\n",
    "    df[column] = fill_missing_qualitative_data(df[column], 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers\n",
    "\n",
    "Having ensured all dataset entries have a value, it is appropriate to remove any **outliers** to avoid training our models with unrepresentative data.\n",
    "\n",
    "#### Quantitative Data\n",
    "\n",
    "To detect outliers in quantitative data, we can start by plotting the box plot of the respective columns as this type of graph is ideal for easily identifying extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the box plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df[quantitative_columns])\n",
    "plt.title('Box Plots of Quantitative Columns')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we can conclude that there are several outliers. However, in order to decide how to deal with them, we need to understand just how many there are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(data: pd.Series, lower_quantile: float) -> pd.Series:\n",
    "    '''Computes the outliers of a given data column.'''\n",
    "    Q1 = data.quantile(lower_quantile)\n",
    "    Q3 = data.quantile(1 - lower_quantile)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    return data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "\n",
    "# calculate the percentage of outliers for each quantitative column\n",
    "print(\"Outliers by column (%):\")\n",
    "\n",
    "for column in quantitative_columns:\n",
    "    outliers = get_outliers(df[column], 0.25)\n",
    "    outliers_percentage = 100 * len(outliers) / len(df[column])\n",
    "\n",
    "    print(f'{column}\\t{round(outliers_percentage, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced, with $Q_{0.25}$ and $Q_{0.75}$ as lower and upper bounds, each quantitative column contains fewer than 5% outliers. Therefore, removing the rows where they apper would not incur a severe data loss for individual columns. However, assuming the worst-case scenario of only one outlier per row, we would be losing over 8% of the dataset, which is not ideal. Consequently, while we will still adopt the outlier removal strategy, we will instead use $Q_{0.10}$ and $Q_{0.90}$ as lower and upper bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'W/ outliers:\\t{df.shape}')\n",
    "\n",
    "for column in quantitative_columns:\n",
    "    # compute the outliers\n",
    "    outliers = get_outliers(df[column], 0.1)\n",
    "\n",
    "    # remove the rows where the outliers appear\n",
    "    df.drop(index=outliers.index, inplace=True)\n",
    "\n",
    "print(f'W/o outliers:\\t{df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitate Data\n",
    "\n",
    "Even though outliers are commonly associated with quantitative data, they can also occur in qualitative data. In this case, the term refers to categories that are atypically infrequent i.e. that appear significantly less than the others.\n",
    "\n",
    "To identify these values, we will compute the percentage of each category per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in qualitative_columns:\n",
    "    # compute the frequency of each category\n",
    "    freqs = pd.DataFrame({\n",
    "        'Count': df[column].value_counts(),\n",
    "        'Frequency (%)': df[column].value_counts(normalize=True) * 100\n",
    "    })\n",
    "\n",
    "    print(f\"\\nCategory Frequency in '{column}'\")\n",
    "    display(freqs.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As depicted, some categories appear very infrequently ($< 0.01\\%$). Despite the desire to include all unique categories for greater fidelity, keeping these rare values in the dataset may introduce noise, as they are unlikely to provide meaningful patterns and can instead take focus away from the dominant trends.\n",
    "\n",
    "An obvious solution to this problem would be to remove the rows containing these categories, which would naturally result in data loss. However, we can take advantage of the fact that we have a category for missing values - `Unspecified` - and use it to encapsulate these atypical categories, thus achieving the desired effect while preserving all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_infrequent_qualitative_data(data: pd.Series, freq_threshold: float)-> pd.Series:\n",
    "    '''Replaces rare categorical values with 'Unspecified'.'''\n",
    "    # compute the frequency of each category\n",
    "    infrequent = data.value_counts(normalize=True)[lambda x : x * 100 < freq_threshold].index\n",
    "\n",
    "    # replace the infrequent values\n",
    "    return data.apply(lambda x: 'Unspecified' if x in infrequent else x)\n",
    "\n",
    "\n",
    "# count the number of unique categories per column\n",
    "before_unique_categories = df[qualitative_columns].nunique()\n",
    "\n",
    "# replace the infrequent values in each column\n",
    "for column in qualitative_columns:\n",
    "    df[column] = replace_infrequent_qualitative_data(df[column], 0.01)\n",
    "\n",
    "# compare the number of unique categories before and after processing\n",
    "print(\"Unique Categories per column:\")\n",
    "pd.DataFrame({\n",
    "    'Before': before_unique_categories,\n",
    "    'After': df[qualitative_columns].nunique()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Correlations in Quantitative Data\n",
    "\n",
    "Even though there are only three quantitative columns, it is still advantageous to compute their correlation, as it may justify removing columns that are highly correlated with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df[quantitative_columns].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated, the maximum correlation between quantitative features is $75\\%$ (`cap-diameter` and `stem-width`). By itself, it could be argued this value does not justify merging the two columns, as it is not particularly high. However, both columns share the same correlation with the third column - `stem-height` - which further corroborates their similarity. Given these two reasons, we believe it is sensible to join the two columns, thus reducing the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new column\n",
    "#df['cap-diameter & stem-width'] = df['cap-diameter'] * df['stem-width']\n",
    "\n",
    "# remove the columns\n",
    "#df.drop(['cap-diameter', 'stem-width'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Qualitative Data\n",
    "\n",
    "As most machine learning algorithms and statistical models can only process numerical input, the final preprocessing step is transforming qualitative data into quantitative data.\n",
    "\n",
    "There are several encoding techniques that achieve this, but we will go with **label encoding**, which merely consists of assigning unique integer values to distinct categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the label encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# apply Label Encoding to all qualitative data\n",
    "for column in df[qualitative_columns]:\n",
    "    df[column] = encoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and target (y)\n",
    "y = df['class']\n",
    "X = df.drop('class', axis=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = DecisionTreeClassifier(random_state=21)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
